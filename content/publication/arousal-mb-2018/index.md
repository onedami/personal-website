+++
title = "Towards Arousal Sensing With High Fidelity Detection of Visual Focal Attention"
date = 2018-06-06T00:00:00
draft = false

# Authors. Comma separated list, e.g. `["Bob Smith", "David Jones"]`.
authors = ["Oludamilare Matthews", "Markel Vigo", "Simon Harper"]

# Publication type.
# Legend:
# 0 = Uncategorized
# 1 = Conference paper
# 2 = Journal article
# 3 = Manuscript
# 4 = Report
# 5 = Book
# 6 = Book section
publication_types = ["1"]

# Publication name and optional abbreviated version.
publication = "12th International Conference on Methods and Techniques in Behavioral Research (2018)"
publication_short = "Measuring Behaviour 2018"

# Abstract and optional shortened version.
abstract = "Measuring emotions objectively in human-computer interaction is complicated because it involves selecting the appropriate detection mechanism, developing the computational technique to analyse it, and evaluating the correctness of the technique. Existing solutions are either manual, multimodal (requiring multiple sensors), have latency in their response, or do not provide context to why the participant experiences certain emotions. Our approach is an algorithm that measures physiological arousal through analysis of pupillary response and relating it to participants’ focal attention. We built our algorithm through a data-driven approach by doing a secondary analysis of two independent datasets. The algorithm works by sensing an increase in arousal through peak detection of the pupil size and compounding the magnitude of this increase with the time spent on the corresponding area of interest. Our contribution to affect detection is that our detection mechanism is unimodal (eye-tracking) and unobtrusive, yet senses arousal with added information about participants’ focal attention when they experienced a certain measure of arousal. In its preliminary evaluation, we used Stroop’s effect to elicit multiple states of arousal on 19 participants. We found a moderate correlation, r(76)=.61, p<.01 between the induced states of arousal and our algorithm’s computed level of arousal. This result suggests that our algorithm could be used to complement existing research methods in usability, UX and studies of visual research behaviour. The committee for ethics at the University of Manchester approved this study."
abstract_short = "We built our algorithm to sense arousal through a data-driven approach by doing a secondary analysis of two independent datasets. The algorithm works by sensing an increase in arousal through peak detection of the pupil size and compounding the magnitude of this increase with the time spent on the corresponding area of interest. We found a moderate correlation, r(76)=.61, p<.01 between the induced states of arousal (using Stroop's effect) and our algorithm’s computed level of arousal. "

# Is this a selected publication? (true/false)
selected = false

# Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["deep-learning"]` references 
#   `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects = ["sensing-arousal"]

# Tags (optional).
#   Set `tags = []` for no tags, or use the form `tags = ["A Tag", "Another Tag"]` for one or more tags.
tags = []

# Links (optional).
url_pdf = "https://www.researchgate.net/publication/328465425_Towards_Arousal_Sensing_With_High_Fidelity_Detection_of_Visual_Focal_Attention"
url_preprint = ""
url_code = ""
url_dataset = ""
url_project = ""
url_slides = ""
url_video = ""
url_poster = ""
url_source = ""

# Custom links (optional).
#   Uncomment line below to enable. For multiple links, use the form `[{...}, {...}, {...}]`.
# url_custom = [{name = "Custom Link", url = "http://example.org"}]

# Digital Object Identifier (DOI)
doi = ""

# Does this page contain LaTeX math? (true/false)
math = true

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
[image]
  # Caption (optional)
  caption = ""

  # Focal point (optional)
  # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
  focal_point = ""
+++
