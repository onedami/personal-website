+++
title = "Unobtrusive Arousal Detection on the Web Using Pupillary Response"
date = 2019-09-26T00:00:00
draft = false

# Authors. Comma separated list, e.g. `["Bob Smith", "David Jones"]`.
authors = ["Oludamilare Matthews", "Alan Davies", "Markel Vigo", "Simon Harper"]

# Publication type.
# Legend:
# 0 = Uncategorized
# 1 = Conference paper
# 2 = Journal article
# 3 = Manuscript
# 4 = Report
# 5 = Book
# 6 = Book section
publication_types = ["2"]

# Publication name and optional abbreviated version.
publication = "International Journal of Human-Computer Studies"
publication_short = "IJHCS '19"

# Abstract and optional shortened version.
abstract = "Arousal detection has been used as a proxy to sense frustration, cognitive load, anxiety and stress, which are relevant to user experience. The mechanisms provide limited potential for widespread use beyond the lab. We used eye-tracking to capture pupillary response and gaze behaviour during user interaction. Pupillary response is used to sense changes in arousal while gaze analysis reveals the usersâ€™ focal attention during moments of increased arousal. A controlled study was run using our approach to detect arousal on the web. Participants (N=40) were presented with four tasks on the web. Each participant carried out each task in a normal and disruptive mode (modified to induce frustration). For ecological validity, we focused on frustration induced arousal, using common causes of frustration during user interaction. Results suggest that our approach is able to discriminate between frustrating tasks and normal tasks to a large effect. We discuss how arousal sensing opens up research avenues for usability and accessibility testing in the lab and the potential for use in naturalistic settings."
abstract_short = "We evaluated our arousal sensing algorithm on frustration-induced arousal. Our algorithm is able to discriminate between normal moments and frustration. We use unimodal sensor for ecological validity and potential for widespread use. We used ecologically valid stimuli to support its applicability on the web"

# Is this a selected publication? (true/false)
selected = false

# Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["deep-learning"]` references 
#   `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects = ["sensing-arousal"]

# Tags (optional).
#   Set `tags = []` for no tags, or use the form `tags = ["A Tag", "Another Tag"]` for one or more tags.
tags = []

# Links (optional).
url_pdf = "https://doi.org/10.1016/j.ijhcs.2019.09.003"
url_preprint = ""
url_code = ""
url_dataset = ""
url_project = ""
url_slides = ""
url_video = ""
url_poster = ""
url_source = ""

# Custom links (optional).
#   Uncomment line below to enable. For multiple links, use the form `[{...}, {...}, {...}]`.
# url_custom = [{name = "Custom Link", url = "http://example.org"}]

# Digital Object Identifier (DOI)
doi = "doi.org/10.1016/j.ijhcs.2019.09.003"

# Does this page contain LaTeX math? (true/false)
math = true

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
[image]
  # Caption (optional)
  caption = ""

  # Focal point (optional)
  # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
  focal_point = ""
+++
